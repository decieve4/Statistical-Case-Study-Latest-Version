Imagine a vast circular chamber, with walls covered in a towering painted map of planet Earth. Picture this hall ‘like a theatre, except that the circles and galleries go right round through the space usually occupied by the stage’. Enormous rings of tiered seating circle its outer walls. Imagine that working in these seats are 64,000 ‘computers’ – humans doing calculations – each preparing a different weather forecast for their designated geography. And in the middle of the hall, on a large pulpit at the top of a tall multistorey pillar, stands the ‘man in charge’, who coordinates the scattered weather calculations from his computers into a global forecast like a ‘conductor of an orchestra’. This ‘forecast factory’ was the dream of the 20th-century English mathematician and meteorologist Lewis Fry Richardson. Following hundreds of pages of equations, velocities and data in his prosaically titled book Weather Prediction by Numerical Process (1922), he asks the reader to indulge him: ‘After so much hard reasoning, may one play with a fantasy?’ For Richardson, one of the main limitations on weather forecasting was a lack of computational capacity. But through the fantasy he could ignore practical problems and bring an entire planet into focus. His ‘factory’ saw once-scattered local observations merging into a coherent planetary system: calculable, predictable, overseen and singular. Richardson died in 1953, the year IBM released the first mass-produced electronic computer. Though his factory never materialised exactly as he imagined it, his dream of a calculable planet now seems prophetic. By the 1960s, numerical calculation of global weather conditions had become a standardised way of recording changes in the atmosphere. Clouds and numbers seemed to crowd the sky. Since the 1960s, the scope of what Richardson called weather prediction has expanded dramatically: climate models now stretch into the deep past and future, encompassing the entirety of the Earth system rather than just the atmosphere. What is startling about this is not that our technical abilities have exceeded Richardson’s wildest dreams but the unexpected repercussions of the modern ‘forecast factory’. The calculable, predictable, overseen and singular Earth has revealed not only aeons of global weather, but a new kind of planet – and, with it, a new mode of governance. The planet, I argue, has appeared as a new kind of political object. I’m not talking about the Sun-orbiting body of the Copernican revolution, or the body that the first astronauts looked back upon in the 1960s: Buckminster Fuller’s ‘Spaceship Earth’, or Carl Sagan’s ‘lonely speck’. Those are the planets of the past millennium. I’m talking about the ‘planet’ inside ‘planetary crisis’: a planet that emerges from the realisation that anthropogenic impacts are not isolated to particular areas, but integrated parts of a complex web of intersecting processes that unfold over vastly disparate timescales and across different geographies. This is the planet of the Anthropocene, of our ‘planetary emergency’ as the UN secretary-general António Guterres called it in 2020. The so-called planetary turn marks a new way of thinking about our relationship to the environment. It also signals the emergence of a distinct governable object, which suggests that the prime political object of the 21st century is no longer the state, it’s the planet. For those closely following contemporary environmental science and politics, this is nothing new. The need to stay within the boundaries of the planet became a flashpoint in the 1970s after the Club of Rome published its report The Limits to Growth (1972). The report suggested that unlimited economic growth on a planet with finite resources would prove unsustainable in the long term. Since then, rather than ‘limits’, the crucial framework for global environmental governance has become ‘planetary boundaries’, a term coined in 2009 by the Swedish scientist Johan Rockström and a team of researchers. This change is emblematic of how global, social, economic and technological relations have become conflated with the physical properties of the planet itself. Modelling the outcome of the entangled and rapidly declining relationship between humans and Earth has become a key task for scientists across a range of fields, which has led to a blurring of scientific and political futures. The closer this relationship gets, the harder it becomes to disentangle the two from each other. For someone like myself, born in the mid-1990s, models of planetary conditions have almost become images of the future itself. The planet is suddenly everywhere. Planning, designing, research, strategising – even the grounds for thinking itself – are now adjusted to the planetary scale. As this scale becomes ubiquitous, it can sometimes be difficult to see that this form of the planet has a history of its own. If the planet just recently resurfaced as a new kind of environmental category, where did it come from? The recently resurfaced planet, I argue, can be traced back to fantasies like Richardson’s. Contemporary planetary governance relies on a specific trajectory of planetary monitoring. Even though efforts to map and measure global space have a far longer history – not least as a primary vehicle of Western colonialism and imperialism – the efforts to monitor and govern planetary dynamics have also relied on a particular history of knowing, seeing and measuring the planet. Thinking of the planet as an interconnected system required quantified and centralised approaches as well as a wide set of scientific instruments and technologies. A specific kind of planet was able to emerge, not by itself, but through the interconnected histories of geopolitics, technology and grand visions of planetary monitoring. Collection of planetary-scale data did not happen in a vacuum: it was mobilised by geopolitical ambition Technically, the beginning of a gradual transition from planetary monitoring to planetary governance began during the postwar era, as connections tightened between the planetary and the global. But even before that time, before the technological and scientific means to monitor planetary dynamics, the conceptual framework for quantifying a dynamic planet was starting to be sketched out. Richardson’s 100-year-old fantasy did not consider the entirety of Earth’s processes (only weather conditions unfolding in real time), but his approach is surprisingly contemporary. Much like his human ‘computers’ reporting weather data to the ‘man in charge’ as they worked along the walls of the forecast factory, today a wide range of scientific fields provide the raw materials for climate modellers. Though the data now spans millennia, the structuring logics of Richardson’s factory have remained intact. Over time, new numerical problems were added to the forecast factory framework and, as the prospect of human impact on planetary dynamics grew more likely, the work of the factory expanded into the domains of politics and history. The data was not just a series of numbers waiting to be plucked from the land, sea and sky. It was made and mobilised. To monitor the entire planet across space and time, researchers in the mid-20th century understood they needed to translate natural phenomena – including tree rings and ice cores – into data that could be synchronised and compared through models. The Earth sciences, such as oceanography, glaciology and meteorology, played a key role in this translation work, but the collection of planetary-scale data did not happen in a vacuum: it was mobilised by the geopolitical ambition to extend military power to the most remote parts of the planet. Tracing the origins of a monitored planet takes us from fantasies of forecast factories to vast material technological networks, Cold War fears and climate histories hidden deep inside the polar ice sheets. The International Geophysical Year (IGY), which started in 1957, is a prime example of how this development took place. The IGY was a remarkable scientific and diplomatic endeavour: by involving 60,000 scientists from 67 countries, it functioned as a kind of ‘scientific Olympics’. The goal was to construct a coherent understanding of how the planet functioned on a geophysical level and how its many components interacted. Remote geographies, such as the polar regions, the ocean floor and the upper atmosphere were drawn into a bourgeoning understanding of the planet as interconnected and dynamic. Technology was at the heart of the IGY: the event was accompanied by a surge of new devices for sensing, sampling and recovering parts of Earth that had previously been inaccessible. Much of this technology emerged from Cold War geopolitics and military funding of Earth sciences. The IGY began in July 1957 and, as it progressed, scientists were given access to a growing parade of scientific instruments: satellites, rockets, weather balloons and radar equipment could be directed towards the atmosphere and beyond; acoustic sonar technologies, current meters, magnetometers and deep-coring devices opened up new domains of study in the oceans; and ice-core drills, crevasse detectors and seismographs did the same for the polar regions. Even though these instruments worked in different ways and applied to different geographies, they produced the same result: numbers. The IGY, with its overarching goal to study the planet as a ‘single physical system’, necessitated a kind of elemental translation. Ice, rock, soil, air and water were measured and turned into numbers, which in turn could form quantified ways of knowing planetary-scale dynamics. In the years after the IGY, the gathered data were stored on microfilm in countries of the World Data Center system, an institution created to coincide with the event and to facilitate the global spread of the research results. Even though this was easier said than done (this was, after all, during the Cold War, and geophysical data was always of interest to the military), the data centres formed a predigital rendition of what would later become digital climate databases. In storage facilities across the world, a quantified version of the planet began to take shape, rolled up on mile-after-mile of microfilm. Geophysical optimism became contrasted with an awareness of ecological fragility – and, potentially, disaster In contrast to contemporary climate models, the IGY did not give primacy to human impact. In fact, political matters of any kind were kept as implicit as possible: the IGY was a dual enterprise, at once scientific and military, but the latter was for diplomatic reasons that were not stated explicitly. The non-political nature of the project was constantly reiterated by Western scientists as they pointed to the universal appeal of acquiring new planetary-scale knowledge that transcended Cold War politics. Potential controversial study objects, such as radioactive fallout and anthropogenic climate change, had to be framed as technological rather than political issues. Rather than being seen as problems for societies to deal with, which might open the door to diplomatic and political issues, changes in planetary dynamics due to human impact were framed as large-scale ‘experiments’. An optimistic idea about humanity’s role in planetary affairs took hold. At the time, the ways that fossil-fuel combustion affected climate dynamics became ‘perhaps the greatest geophysical experiment in history’, in the words of Roger Revelle, a leading US oceanographer and organiser behind the IGY. With the rise of environmentalism in the 1960s, and broader concerns about anthropogenic environmental degradation, the geophysical optimism of the IGY became contrasted with an awareness of ecological fragility – and, potentially, disaster. Many of the environmental concerns that emerged at that time stemmed from scientific traditions such as ecology, biology and plant physiology, and a geophysical conceptualisation of the planet remained beyond the scope of environmental consciousness. Given the close ties between military funding and geophysical research, environmental issues were left to other disciplines, and the utility of geophysics was primarily conceptualised in geopolitical terms and as a military strategic tool. However, this was about to change. The enormous processes and vast geological timescales of geophysics would begin intersecting with the political urgency of environmentalism and an understanding of a perilous relationship between humanity and the planet began taking shape. In 1966, on a US army base named Camp Century in northwestern Greenland, a team of scientists took one step in that direction. Hidden deep inside the Greenland ice sheet, the base was a US Cold War project par excellence: fuelled by a nuclear reactor, situated in a geopolitically strategic site in the Arctic, and well connected to the technological and infrastructural force of the US Army. It was also in the process of being abandoned. A cutaway drawing view of Camp Century Nuclear Laboratory, Greenland (c1960-61). Courtesy the US Army Corps of EngineersAs military interest moved to other parts of the world, the Danish geophysicist Willi Dansgaard saw his chance to utilise the Greenland base to drill far further down into the ice than had previously been possible. His goal was to recover cores: long, cylindrical cut-outs of the ice sheet in which layers of ice had gathered over centuries or even millennia. Using the recently developed technique of isotope dating, Dansgaard could study the stratigraphy in the ice cores to uncover the composition of the atmosphere at the time the ice froze. A new kind of conceptualisation of ice was taking form: by approaching the ice sheet as a vertical space, it could function as an archive of past climates, a frozen repository of planetary histories. Earlier generations of glaciologists had often spoken of ice as a ‘calendar’ that could be studied in real time because glaciers respond to changing climates around them. Later, these ‘calendars’ would be understood as ‘archives’ – a change in temporal metaphor indicative of a larger conceptual shift in the relation between ice and history, where the slow bodies of the glaciers were increasingly seen as proxies for rapid planetary change. Engineers capture an ice core at Camp Century, Greenland, 1960-61. Courtesy the ERDC Cold Regions Research and Engineering LaboratoryAt Camp Century, Dansgaard finally had the technological means to try his dating methods on a larger scale. As a graduate student in Copenhagen, he had used an old beer bottle and a funnel to gather rainwater in his garden in order to study ‘old water’. Having access to a technologically advanced military base inside the Greenland ice sheet – arguably a better place for the study of old water – he could vastly scale up his research. In 1966, Dansgaard and his team drilled all the way through the ice sheet down to bedrock, 1,387 metres below them. The ice core they were able to recover was far longer than any previous core and produced a climate history dating back 100,000 years. For reference, the attempts to drill for ice cores during the IGY in 1957-58 had produced records of only 900 years. Dansgaard’s core revealed the past of a planet that had undergone dramatic shifts during its history, with large variabilities and rapidly changing climates. If the planet had undergone rapid shifts in the past, it could do so again The scientists left Camp Century shortly after the drilling because it was gradually being crushed by the slow movements of the ice sheet (the facility has only recently begun resurfacing as climactic changes cause the ice above to melt). In a sense, the real work began once they left. Chopped up into small pieces, the ice core was circulated around research facilities in the US and Denmark, and the massive amounts of data it held suggested new research questions: what could ice cores be good for? Dansgaard had plenty of ideas. ‘The development of the ice core drilling technique has led to a broad variety of studies reaching far beyond glaciology itself,’ he wrote in 1973, and listed a wide range of disciplines – from solar physics to meteorology and atmospheric chemistry – that could benefit from ice-core studies. He also began speculating whether ice cores could be used to predict future climatic conditions and form a baseline against which human impact could be measured. He particularly appealed to the growing community of climate modellers, and asserted that they should use his ice cores to diagnose ‘the processes that cause climatic changes’ and to check the validity of their models. One can almost imagine Dansgaard showing up at Richardson’s forecast factory, an ice core under his arm, taking his place along the sides of the enormous theatre and disclosing an entirely new set of numbers to the man on the central pulpit. This increase in the temporal scope of Earth’s 20th-century forecast factory added an additional dimension to the understanding of the planet. The planet stretched out in time and could now be considered a place with a rich and dramatic climate history. It also raised the stakes for the future: if the planet had undergone rapid shifts in the past, it could do so again. In the early 1970s, Dansgaard was far from alone in his ambition to bring together long, planetary timescales with the surging scientific field of climate modelling. Ice cores were lined up along with other climate archives – deep-sea sediments, ancient pollen samples, corals, tree rings – that could now be translated into one coherent climate system. But the archives alone were not enough: while Dansgaard and his colleagues were drilling through the Greenland ice sheets, a new generation of meteorologists and climate modellers had developed increasingly advanced methods to calculate and predict future climate conditions. What began as an ambition to predict short-term changes in the weather, an ambition shared by Richardson, was growing temporally and spatially. The first General Circulation Models (GCMs), developed to quantify atmospheric processes on monthly or seasonal timescales, were produced in the 1960s. But these models were still rudimentary, produced by small teams, and could give only fragmented and partial answers to the questions the modellers asked them. New questions were also emerging. As the leading climate scientist William Welch Kellogg wrote in 1971, ‘the haunting realisation’ that humanity could alter Earth’s climate had become ‘one of the most important questions of our time’. In the years after Kellogg’s realisation, different components of the quantified planet began coming together: the coordinated global data gathered by the IGY, the deep timescales made visible in ice cores and other climate archives, and the growing concern for human impact on planetary dynamics were drawn into one coherent model. While the GCMs were able to ‘calculate’ the immensely complex atmosphere, a new generation of models aimed to conceptualise the entire Earth as a system consisting of interacting components. If each of these components – whether oceans, atmosphere or polar regions – could be quantified and modelled in relation to each other, the argument went, it could be possible to produce models of the Earth system. Making models on such a large scale, involving so many metrics and components, also necessitated a great deal of pragmatism. Computing power is not infinite. Each process entering the model had to be reduced to a manageable scale, translating complex local variabilities into standardised algorithms. As Earth System Models (ESMs) gradually replaced GCMs, the ambition of modelling the planet grew in scope but remained surprisingly modest: the goal became far more wide reaching but did not necessitate any dramatic increase in data collection. The late Francis Bretherton, a mathematician and former chairman of NASA’s Earth System Science Committee, outlined the philosophy of his research agenda in an interview in Science in 1986: ‘Many of the observations we need are already being made for other reasons, such as weather forecasting.’ The difference from previous approaches, he believed, was intellectual rather than technological: ‘It’s more an attitude of mind. We want to make sure that we go the extra mile – that we cover everything.’ The perspective was shifting. It appeared that the models could encompass far more phenomena than had previously seemed possible. Planetary monitoring was becoming planetary governance The unifying agenda outlined by Bretherton and his colleagues sought to bring different processes – geological, geochemical, biological, political – into one coherent framework. The role of human impact also became seen as a factor that affected the Earth system, but it was simultaneously naturalised as just one of many forces interacting on the planet. In visualisations and diagrams of the Earth system, a humble box with ‘human activity’ written on it appeared next to boxes representing other planetary processes. Humanity appeared as a monolith, a unified force like any other geophysical category. The insertion of human activities into Earth system modelling marked a formalisation of what had been long in the making: the past and future of human life was one of many planetary timescales that the newfound Earth system science was tracking. The stability of the system did not just depend on natural forces, but also on political decisions and environmental regulations. Planetary monitoring was becoming planetary governance. The recent turn towards the planetary must be understood as the outcome of a longer scientific history of conceptualising planetary dynamics and fantasies of planetary monitoring. The planet has, in one sense, always been here but, in another, it has just recently appeared. In the century since Richardson shared his fantasy, an immense mobilisation of science and technology, as well as political and financial interest in planetary dynamics, has fundamentally altered the relationship between Earth and its human inhabitants. At the same time, the human pressure on the Earth system has reached unprecedented, and potentially disastrous, proportions. The knowledge of the planet as an interconnected system has co-evolved with the acceleration of human impacts on the system. It’s somewhat surprising how well Richardson’s model of the forecast factory has held up, given the dramatic expansion of knowledge in the past 100 years and the insight that humanity is not just living with climate, but effectively interfering in it. Richardson was merely fantasising about improving short-term weather prediction, which at the time seemed like a wild dream, and yet the same model of gathering data from across the world to produce synchronous models of planetary dynamics has remained remarkably intact. Early numerical weather prediction produced highly scalable models for calculating the weather, which were gradually scaled up to encompass an ever-increasing number of metrics and variables – increasing even now. Global coordinated scientific projects, from the International Geophysical Year to Dansgaard’s ice cores, are emblematic of how new categories and datasets have entered a pre-existing framework. With the establishment of Earth system science in the 1980s, the forecast factory expanded into a multitude of processes, as well as into the domain of politics. While Richardson was living at a time when human impact was not a factor to reckon with, his fantasy has remained intact even as humanity steps into the foreground of the very processes the factory aims to forecast. With the planet emerging as this century’s prime political object, understanding its history can help us recognise how the planetary, despite its seemingly unequivocal nature, carries a politics of its own. This new Earth – calculable, predictable, overseen and singular – did not rise in a vacuum. 